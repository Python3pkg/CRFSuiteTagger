{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple chunking example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common sequence tagging tasks is chunking. Here we will download the CoNLL-2000 data and use it to train a chunking model. Make note that the terminology used here is relevant for chunking, but the it may vary if the tagger is applied to another task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip, os, wget\n",
    "\n",
    "def extract(fp):\n",
    "    f = gzip.open(fp, 'rb')\n",
    "    with open(fp[:-3], 'w') as fh:\n",
    "        fh.write(f.read())\n",
    "    f.close()\n",
    "\n",
    "try:\n",
    "    os.makedirs('data')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs('thesauri')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "train_url = 'http://www.cnts.ua.ac.be/conll2000/chunking/train.txt.gz'\n",
    "test_url = 'http://www.cnts.ua.ac.be/conll2000/chunking/test.txt.gz'\n",
    "stanford_clusters_url = 'http://nlp.stanford.edu/software/egw4-reut.512.clusters'\n",
    "\n",
    "train = wget.download(train_url, out='data/train.txt.gz')\n",
    "test = wget.download(test_url, out='data/test.txt.gz')\n",
    "\n",
    "stanford = wget.download(\n",
    "    stanford_clusters_url,\n",
    "    out='thesauri/egw4-reut.512.clusters'\n",
    ")\n",
    "\n",
    "extract(train)\n",
    "extract(test)\n",
    "\n",
    "# clean up\n",
    "os.remove(train)\n",
    "os.remove(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, we need to set up the configuration for the tagger. Usually these are loaded from a text file into a `ConfigParser` object, but here we parse them from a string for presentation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ConfigParser, StringIO\n",
    "\n",
    "cfg_str = \"\"\"[tagger]\n",
    "# Training data path\n",
    "train=data/test.txt\n",
    "\n",
    "# Testing data path\n",
    "test=data/test.txt\n",
    "\n",
    "# Model path\n",
    "model=tmp/model\n",
    "\n",
    "# Feature vector\n",
    "ftvec=word:[-3:3];can:[-3:3];cls:[0];short\n",
    "\n",
    "# column separator in input (and output) file(s)\n",
    "tab_sep=\\s\n",
    "\n",
    "# Column pattern\n",
    "# [pos <form, postag>, chunk <form, postag, chunktag>]\n",
    "cols=chunk\n",
    "\n",
    "# Label column name\n",
    "label_col=chunktag\n",
    "\n",
    "# Evaluation function [pos, conll]\n",
    "# Note: the evaluation functions are not constrained by tagset. However, the\n",
    "# conll and bio evaluation functions work only with BIO or BIOSE tagsets.\n",
    "eval_func=bio\n",
    "\n",
    "# Name for the guess label column\n",
    "guess_label_col=guesstag\n",
    "\n",
    "[resources]\n",
    "# Stanford clusters\n",
    "cls=thesauri/egw4-reut.512.clusters\n",
    "\n",
    "[crfsuite]\n",
    "# coefficient for L1 penalty\n",
    "c1=0.80\n",
    "# coefficient for L2 penalty\n",
    "c2=1e-3\n",
    "# stop earlier\n",
    "max_iterations=100\n",
    "# include transitions that are possible, but not observed\n",
    "feature.possible_transitions=True\n",
    "\"\"\"\n",
    "\n",
    "sio = StringIO.StringIO(cfg_str)\n",
    "cfg = ConfigParser.ConfigParser()\n",
    "cfg.readfp(sio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a model using that configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "--------------------------------------------------------\n",
       "--------------------------------------------------------\n",
       "Total ==> pre: 95.69, rec: 94.87, f: 95.28 acc: n.a.\n",
       "--------------------------------------------------------\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from crfsuitetagger.tagger import CRFSTagger\n",
    "from crfsuitetagger.utils import parse_tsv, export\n",
    "\n",
    "c = CRFSTagger(cfg)\n",
    "c.train()\n",
    "r, d = c.test()\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reusing a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model` option in the tagger configuration sets the location where the newly created model should be dumped. Using that location, one can load the model later on and use it for tagging more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('Rockwell', 'NNP', 'B-NP', 'B-NP', 28),\n",
       "       ('International', 'NNP', 'I-NP', 'I-NP', -1),\n",
       "       ('Corp.', 'NNP', 'I-NP', 'I-NP', -1),\n",
       "       (\"'s\", 'POS', 'B-NP', 'B-NP', -1),\n",
       "       ('Tulsa', 'NNP', 'I-NP', 'I-NP', -1)], \n",
       "      dtype=[('form', 'S60'), ('postag', 'S10'), ('chunktag', 'S10'), ('guesstag', 'S10'), ('eos', '<i4')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = CRFSTagger(mp='tmp/model')\n",
    "data = parse_tsv('data/test.txt', cols='chunk', ts=' ')\n",
    "d = c.tag(data=data)\n",
    "export(d, open('tmp/chunk_output.txt', 'w'), cols='chunk')\n",
    "d[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature vector templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central piece in the configuration of the tagger is the feature vector template. It is the sample pattern used for generating the feature vectors of every observation. For the model we just used, we set up a feature vector template with the following features:\n",
    "\n",
    "    word:[-3:3];can:[-3:3];cls:[0];short\n",
    "\n",
    "Each feature name in the template, e.g. `word`, is in fact a series of features generated from a context window. The context window is defined by the numbers in brackets, for example `[-3:3]`. Features are separated by semi-columns. Some features require additional parameters, like n-grams, for example, need to be specified as bigrams, trigrams, etc. These parameters are specified after the window brackets separated with commas like this:\n",
    "    \n",
    "    npos[-1:1],2\n",
    "\n",
    "We can improve chunking by adding another window feature based on part-of-speech tags.\n",
    "\n",
    "    word:[-3:3];pos:[-3:3];can:[-3:3];cls:[0];short\n",
    "\n",
    "To do that, we just set the value of the `ftvec` option in the tagger configuration to the new feature vector template, and re-train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "--------------------------------------------------------\n",
       "--------------------------------------------------------\n",
       "Total ==> pre: 96.9, rec: 96.5, f: 96.7 acc: n.a.\n",
       "--------------------------------------------------------\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.set('tagger', 'ftvec', 'word:[-3:3];pos:[-3:3];can:[-3:3];cls:[0];short')\n",
    "\n",
    "c = CRFSTagger(cfg)\n",
    "c.train()\n",
    "r, d = c.test()\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CRFSuiteTagger` allows the easy integration of new types of features through custom functions following a simple pattern. The feature function needs to have four leading parameters named in a particular way, apart from whatever other parameters it needs. It is recommended that the functions take `*args` and `**kwargs` parameters for safety reasons. The obligatory parameters are `data`, `i`, `cols`, and `rel`. Each of them has a vital importance for the way context features are generated. \n",
    "\n",
    "Here is an example function that we can use to overwrite the existing `word` function in order to introduce noise into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word(data, i, cols, rel=0, *args, **kwargs):\n",
    "    \"\"\"Generates a feature based on the `form` column, but replaces some\n",
    "    prepositions with a placeholder <preposition>.\n",
    "\n",
    "    **FEATURE GENERATION FUNCTION**\n",
    "\n",
    "    :param data: data\n",
    "    :type: np.recarray\n",
    "    :param i: focus position\n",
    "    :type i: int\n",
    "    :param cols: column map\n",
    "    :type cols: dict\n",
    "    :param rel: relative position of context features\n",
    "    :type rel: int\n",
    "    :return: feature\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if 0 <= i + rel < len(data):\n",
    "        form = data[i + rel][cols['form']]\n",
    "    else:\n",
    "        form = None\n",
    "    if form in ['to', 'from', 'with', 'in', 'over', 'by', 'through']:\n",
    "        form = '<preposition>'\n",
    "    return 'w[%s]=%s' % (rel, form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of feature functions is passed to the constructor of the `CRFSuiteTagger` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "--------------------------------------------------------\n",
       "--------------------------------------------------------\n",
       "Total ==> pre: 96.78, rec: 96.6, f: 96.69 acc: n.a.\n",
       "--------------------------------------------------------\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from crfsuitetagger.tagger import CRFSTagger\n",
    "from crfsuitetagger.utils import parse_tsv, export\n",
    "\n",
    "c = CRFSTagger(cfg, fnx=[word])\n",
    "c.train()\n",
    "r, d = c.test()\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the new `word` feature function brought down the precision of the model with .12%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('Rockwell', 'NNP', 'B-NP', 'B-NP', 28),\n",
       "       ('International', 'NNP', 'I-NP', 'I-NP', -1),\n",
       "       ('Corp.', 'NNP', 'I-NP', 'I-NP', -1),\n",
       "       (\"'s\", 'POS', 'B-NP', 'B-NP', -1),\n",
       "       ('Tulsa', 'NNP', 'I-NP', 'I-NP', -1)], \n",
       "      dtype=[('form', 'S60'), ('postag', 'S10'), ('chunktag', 'S10'), ('guesstag', 'S10'), ('eos', '<i4')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = None\n",
    "c = CRFSTagger(mp='tmp/model')\n",
    "data = parse_tsv('data/test.txt', cols='chunk', ts=' ')\n",
    "d = c.tag(data=data)\n",
    "export(d, open('tmp/chunk_output.txt', 'w'), cols='chunk')\n",
    "d[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the serialisation of the custom feature functions, no packages (including those part of the distribution) can be used, nor should they. If you are doing any more complicated computation during feature generation then probably you are doing something wrong, and you probably need to pre-compute a resource and load it as a parameter (see how cluster features work in `crfsuitetagger.features.ft_cls` and `crfsuitetagger.readers.cls`). If you really, really have to use some package you can hack it by importing it in the function -- bad style, worse performance, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
